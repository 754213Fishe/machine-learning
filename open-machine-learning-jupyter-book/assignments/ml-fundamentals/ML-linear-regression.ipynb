{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e5f991-6918-4b4f-89f5-ce606f3d54f7",
   "metadata": {},
   "source": [
    "# ML linear regression\n",
    "\n",
    "In this assignment, we will predict the price of a house by its region, size, number of bedrooms, etc.\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de1cd4-40a8-485e-a037-3ef804a9fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import urllib.request \n",
    "\n",
    "import pytest\n",
    "import ipytest\n",
    "import unittest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "df = pd.read_csv(\"../../assets/data/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0efd8a1-31ec-4f80-9be3-182d5268ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf5a91-d44f-41eb-84c8-1084144581c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f7454-6a71-47a3-a993-19fc5b12d5a7",
   "metadata": {},
   "source": [
    "Features | Informations\n",
    "--- | --- |\n",
    "`longitude` | A measure of how far west a house is; a higher value is farther west\n",
    "`latitude` | A measure of how far north a house is; a higher value is farther north\n",
    "`housing_median_age` | Median age of a house within a block; a lower number is a newer building\n",
    "`total_rooms` | Total number of rooms within a block\n",
    "`total_bedrooms` | Total number of bedrooms within a block\n",
    "`population` | Total number of people residing within a block\n",
    "`households` | Total number of households, a group of people residing within a home unit, for a block\n",
    "`median_income` | Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "`median_house_value` | Median house value for households within a block (measured in US Dollars)\n",
    "`oceanProximity` | Location of the house w.r.t ocean/sea\n",
    "\n",
    "Source: Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff182b24-db5c-48d4-8a71-3071db405ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39369fc-b2f0-466b-b97d-acefaa0c4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0689f35-bb53-4b1c-ad1a-a88c4ddd11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ecedb-3b7a-482b-818d-30931c2aa41d",
   "metadata": {},
   "source": [
    "So, we have 20640 data points and 10 features. In those 10 features, 9 features are input features and the feature `median_house_value` is the target variable/label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e12839-6858-4356-9b8e-c07e8304636a",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "### Task 1: Exploratory data analysis\n",
    "\n",
    "#### Split the data set into a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d56ff0-4a39-4f37-83f7-9eb210e2359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(cal_data, test_size=0.1,random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57491d38-2fa4-4ed7-a061-a7ff382b6ce2",
   "metadata": {},
   "source": [
    "#### Checking data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e9928-3613-409d-b406-00b8f7110c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc47a7-a866-4b98-af63-732db6eb4b0c",
   "metadata": {},
   "source": [
    "#### Checking missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e532df-f168-461a-b1cd-c5f9010cd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33766f-92fe-4cd3-b51c-f011d3bdb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Percentage of missing values in total_bedrooms is: {}%'.format(train_data.isnull().sum()['total_bedrooms'] / len(train_data) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c5d5f-be7e-4694-9450-b1c5c862003a",
   "metadata": {},
   "source": [
    "#### Checking values in the categorical feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6c4fa-7b80-4043-88f2-a8c2b26729aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27acf7-b368-4567-b583-b33a2b8c3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=train_data, x='ocean_proximity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbc4f0-e8a0-4730-aefe-93b9eb68469e",
   "metadata": {},
   "source": [
    "#### Checking Correlation Between Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39843a62-e4e5-4562-9925-7e2f44c5ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = train_data.corr()\n",
    "correlation['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b329fce-3d35-4f4f-a584-2600da2fad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "sns.heatmap(correlation,annot=True,cmap='crest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe8af2-193a-46cc-a7e3-d61fae2926f8",
   "metadata": {},
   "source": [
    "Some features like total_bedrooms and households are highly correlated. Same things for `total_bedrooms` and `total_rooms` and that makes sense because for many houses, the number of people who stay in that particular house (`households`) goes with the number of available rooms(`total_rooms`) and `bed_rooms`.\n",
    "\n",
    "The other interesting insights is that the `price of the house` is closely correlated with the `median income`, and that makes sense too. For many cases, you will resonably seek house that you will be able to afford based on your income.\n",
    "\n",
    "#### Plotting geographical features\n",
    "\n",
    "Since we have latitude and longitude, let's plot it. It can help us to know the location of certain houses on the map and hopefully this will resemble California map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ffb51-9ac2-4589-866c-466bb104e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "sns.scatterplot(data = train_data, x='longitude', y='latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e449d-8622-45b9-9507-bbf2d2b4fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "sns.scatterplot(data = train_data, x='longitude', y='latitude', hue='median_house_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080357e9-00c6-476a-816d-5aa21fb9fbe7",
   "metadata": {},
   "source": [
    "It makes sense that the most expensive houses are those close to sea. We can verify that with the `ocean_proximity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1364c8-722d-4a28-9844-a3ef96b1abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "sns.scatterplot(data = train_data, x='longitude', y='latitude', hue='ocean_proximity', \n",
    "                size='median_house_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf620448-c4fc-4f9f-84f9-8f9cfb6119d4",
   "metadata": {},
   "source": [
    "Yup, all houses near the ocean are very expensive compared to other areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c08917-7f0a-46e3-8a71-f96e5de7400d",
   "metadata": {},
   "source": [
    "#### Exploring Relationship Between Individual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1e8fa-31f1-4f23-966f-5e59a001b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "sns.scatterplot(data = train_data, x='median_house_value', y='median_income', hue='housing_median_age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e238b8-1c75-4bcd-a381-52143347673b",
   "metadata": {},
   "source": [
    "There are times you want to quickly see different plots to draw insights from the data. In that case, you can use grid plots. Seaborn, a visualization library provides a handy function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8c435-2d2c-4987-8837-554243a0179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8825dc8-22d4-41e5-b367-84ebc3b01703",
   "metadata": {},
   "source": [
    "As you can see, it plots the relationship between all numerical features and histograms of each feature as well. But it's slow...\n",
    "\n",
    "To summarize the data exploration, the goal here it to understand the data as much as you can. There is no limit to what you can inspect. And understanding the data will help you build an effective ML systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd910f-5479-4768-8a26-6fe05849022c",
   "metadata": {},
   "source": [
    "### Task 3: Data preprocessing\n",
    "\n",
    "#### Create the input data and output data for training the machine learning model\n",
    "\n",
    "Since we are going to prepare the data for the ML model, let's create an input training data and the training label, label being `median_house_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd8bae4-dc2d-41cd-9958-95eeaa5c4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_data = train_data.drop('median_house_value', axis=1)\n",
    "training_labels = train_data['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536872a-c420-46b2-84ef-98d816e1a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b7baf-2f6a-4787-92aa-ad670a2fb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1378814-74f2-4c24-8e3a-e69cccb8315b",
   "metadata": {},
   "source": [
    "#### Handling missing values\n",
    "\n",
    "In this example, we will fill the values with the mean of the concerned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498d69f-ba84-4289-810b-89035a860bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to impute all numerical features\n",
    "# Ideally, we would only impute bed_rooms because it's the one possessing NaNs\n",
    "num_feats = training_input_data.drop('ocean_proximity', axis=1)\n",
    "\n",
    "def handle_missing_values(input_data):\n",
    "  \"\"\"\n",
    "  Docstring \n",
    "\n",
    "  # This is a function to take numerical features...\n",
    "  ...and impute the missing values\n",
    "  # We are filling missing values with mean\n",
    "  # fit_transform fit the imputer on input data and transform it immediately\n",
    "  # You can use fit(input_data) and then transform(input_data) or\n",
    "  # Or do it at once with fit.transform(input_data)\n",
    "  # Imputer returns the imputed data as a NumPy array \n",
    "  # We will convert it back to Pandas dataframe\n",
    "\n",
    "  \"\"\"\n",
    "  mean_imputer = SimpleImputer(strategy='mean')\n",
    "  num_feats_imputed = mean_imputer.fit_transform(input_data)\n",
    "  num_feats_imputed = pd.DataFrame(num_feats_imputed, \n",
    "                            columns=input_data.columns, index=input_data.index )\n",
    "\n",
    "\n",
    "  return num_feats_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f3801-24be-43fe-acec-8ad80cfab992",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats_imputed = handle_missing_values(num_feats)\n",
    "num_feats_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32438487-dcb2-40ef-a23b-50999bbb352a",
   "metadata": {},
   "source": [
    "The feature `total_bedroom` was the one having missing values. Looking above, we no longer have the missing values in whole dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff69940-dc1b-4b1d-8ba1-936ad57ce14b",
   "metadata": {},
   "source": [
    "#### Encoding categorical features\n",
    "\n",
    "Categorical features are features which have categorical values. An example in our dataset is `ocean_proximity` that has the following values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e3b5e-78b2-488e-aed0-8e541b8ee518",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_data['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a44d9a-4052-4ffc-95eb-f2f1481ece9e",
   "metadata": {},
   "source": [
    "So we have 5 categories: `<1H OCEAN`, `INLAND`, `NEAR OCEAN`, `NEAR BAY`, `ISLAND`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f2a140-fed7-4d14-9182-688cc393cf8a",
   "metadata": {},
   "source": [
    "##### Mapping\n",
    "\n",
    "Mapping is simple. We create a dictionary of categorical values and their corresponding numerics. And after that, we map it to the categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41192513-343e-4841-973d-e5f823019edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = training_input_data['ocean_proximity']\n",
    "cat_feats.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49fdc7-41f4-443e-8e64-05ad213617c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_map = {\n",
    "      '<1H OCEAN': 0,\n",
    "      'INLAND': 1,\n",
    "      'NEAR OCEAN': 2,\n",
    "      'NEAR BAY': 3, \n",
    "      'ISLAND': 4\n",
    "}\n",
    "\n",
    "cat_feats_encoded = cat_feats.map(feat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907dea5-1f5b-460f-a882-86d009899365",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3a4477-8aa9-4995-b26c-f55d22063e21",
   "metadata": {},
   "source": [
    "Cool, all categories were mapped to their corresponding numerals. That is actually encoding. We are converting the categories (in text) into numbers, typically because ML models expect numeric inputs.\n",
    "\n",
    "##### Handling categorical features with Sklearn\n",
    "\n",
    "Sklearn has many preprocessing functions to handle categorical features. Ordinary Encoder is one of them. It does the same as what we did before with mapping. The only difference is implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c3f7e-8439-4965-83f0-145eb81b7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def ordinary_encoder(input_data):\n",
    "  \n",
    "  encoder = OrdinalEncoder()\n",
    "  \n",
    "  output = encoder.fit_transform(input_data)\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9a521-ecb9-4370-9357-a3cd640120a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats_enc = ordinary_encoder(cat_feats)\n",
    "cat_feats_enc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f3e79e",
   "metadata": {},
   "source": [
    "##### One hot encoding\n",
    "\n",
    "One hot encoding is most preferred when the categories are not in any order and that is exactly how our categorical feature is. This is what I mean by saying unordered categories: If you have 3 cities and encode them with numbers (1,2,3) respectively, a machine learning model may learn that city 1 is close to city 2 and to city 3. As that is a false assumption to make, the model will likely give incorrect predictions if the city feature plays an important role in the analysis.\n",
    "\n",
    "On the flip side, if you have the feature of ordered ranges like low, medium, and high, then numbers can be an effective way because you want to keep the sequence of these ranges.\n",
    "\n",
    "In our case, the ocean proximity feature is not in any order. By using one hot, The categories will be converted into binary representation (1s or 0s), and the orginal categorical feature will be splitted into more features, equivalent to the number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2918d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot(input_data):\n",
    "\n",
    "  one_hot_encoder = OneHotEncoder()\n",
    "  output = one_hot_encoder.fit_transform(input_data)\n",
    "  \n",
    "  # The output of one hot encoder is a sparse matrix. \n",
    "  # It's best to convert it into numpy array \n",
    "  output = output.toarray()\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = training_input_data[['ocean_proximity']]\n",
    "\n",
    "cat_feats_hot = one_hot(cat_feats)\n",
    "\n",
    "cat_feats_hot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "837f85a1",
   "metadata": {},
   "source": [
    "Cool, we now have one hot matrix, where categories are represented in 1s or 0s. As one hot create more additional features, if you have a categorical feature having many categories, it can be too much, hence resulting in poor performance.\n",
    "\n",
    "#### Scaling numerical \n",
    "\n",
    "Most machine learning models will work well when given small input values, and best if they are in the same range. For that reason, there are two most techniques to scale features:\n",
    "\n",
    "* Normalization where the features are scaled to values between 0 and 1. And\n",
    "* Standardization where the features are rescaled to have 0 mean and unit standard deviation. When working with datasets containing outliers (such as time series), standardization is the right option in that particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db803cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizing numerical features \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "num_scaled = scaler.fit_transform(num_feats)\n",
    "num_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aba2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardizing numerical features \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "num_scaled = scaler.fit_transform(num_feats)\n",
    "num_scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fec6d907",
   "metadata": {},
   "source": [
    "#### Putting all data preprocessing steps into a single pipeline\n",
    "\n",
    "We are going to do three things:\n",
    "\n",
    "* Creating a numerical pipeline having all numerical preprocessing steps (handling missing values and standardization)\n",
    "* Creating a categorical pipeline to encode the categorical features\n",
    "* Combining both pipelines into one pipeline.\n",
    "\n",
    "##### Creating a numerical features pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e10e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_feats_pipe = Pipeline([\n",
    "                  ('imputer', SimpleImputer(strategy='mean')), \n",
    "                  ('scaler', StandardScaler())         \n",
    "            ])\n",
    "\n",
    "num_feats_preprocessed = num_feats_pipe.fit_transform(num_feats)\n",
    "\n",
    "num_feats_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af238521",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats_pipe.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe502e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats_pipe.steps[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e315bccc",
   "metadata": {},
   "source": [
    "##### Pipeline for transforming categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats_pipe = Pipeline([\n",
    "     ('encoder', OneHotEncoder())                      \n",
    "])\n",
    "\n",
    "cat_feats_preprocessed = cat_feats_pipe.fit_transform(cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99289600",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cat_feats_preprocessed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dfd6529",
   "metadata": {},
   "source": [
    "##### Final data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# The transformer requires lists of features\n",
    "\n",
    "num_list = list(num_feats)\n",
    "cat_list = list(cat_feats)\n",
    "\n",
    "final_pipe = ColumnTransformer([\n",
    "   ('num', num_feats_pipe, num_list),    \n",
    "   ('cat', cat_feats_pipe, cat_list)                        \n",
    "\n",
    "])\n",
    "\n",
    "training_data_preprocessed = final_pipe.fit_transform(training_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(training_data_preprocessed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e390b0f",
   "metadata": {},
   "source": [
    "### Choosing and training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f58fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model.fit(training_data_preprocessed, training_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "354f7f83",
   "metadata": {},
   "source": [
    "Great, that was fast! The model is now trained on the training set.\n",
    "\n",
    "Before we evaluate the model, let's take things little deep.\n",
    "\n",
    "Have you heard of things called weights and bias? These are two paremeters of any typical ML model. It is possible to access the model paremeters, here is how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coef or coefficients are referred to as weights\n",
    "\n",
    "reg_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e1f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept is what can be compared to the bias \n",
    "\n",
    "reg_model.intercept_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7c00094",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c188533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "predictions = reg_model.predict(training_data_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9832d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(training_labels, predictions)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40798de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe().median_house_value['mean']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0f83b6b",
   "metadata": {},
   "source": [
    "#### Model evaluation with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scoring = 'neg_root_mean_squared_error' \n",
    "\n",
    "scores = cross_val_score(reg_model, training_data_preprocessed, training_labels, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf06647",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = -scores\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdf8b27d",
   "metadata": {},
   "source": [
    "You can also use `cross_val_predict` to make a prediction on the training and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea8381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "predictions = cross_val_predict(reg_model, training_data_preprocessed, training_labels, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_cross_val = mean_squared_error(training_labels, predictions)\n",
    "rmse_cross_val = np.sqrt(mse_cross_val)\n",
    "rmse_cross_val "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a221de3",
   "metadata": {},
   "source": [
    "To evaluate the model on the test set, we will have to preprocess the test dat as we preprocessed the training data. This is a general rule for all machine learning models. The test input data must be in the same format as the data that the model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_input_data = test_data.drop('median_house_value', axis=1)\n",
    "test_labels = test_data['median_house_value']\n",
    "\n",
    "\n",
    "test_preprocessed = final_pipe.transform(test_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d916a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = reg_model.predict(test_preprocessed)\n",
    "test_mse = mean_squared_error(test_labels,test_pred)\n",
    "\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f27dfaad",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "Thanks to Nyandwi for creating the open-source course [Linear Models for regression](https://github.com/Nyandwi/machine_learning_complete/blob/main/6_classical_machine_learning_with_scikit-learn/1_linear_models_for_regression.ipynb). It inspires the majority of the content in this chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
