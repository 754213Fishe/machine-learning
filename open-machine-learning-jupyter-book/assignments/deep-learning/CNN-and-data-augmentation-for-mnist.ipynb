{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# CNN and data augmentation for mnist"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"source":["## Accuracy=99.75% using 25 Million Training Images!!\n","It's amazing that convolutional neural networks can classify handwritten digits so accurately. In this kernel, we witness an ensemble of 15 CNNs classify Kaggle's MNIST digits after training on Kaggle's 42,000 images in \"train.csv\" plus 25 million more images created by rotating, scaling, and shifting Kaggle's images. Learning from 25,042,000 images, this ensemble of CNNs achieves 99.75% classification accuracy. This kernel uses ideas from the best published models found on the internet. Advanced techniques include data augmentation, nonlinear convolution layers, learnable pooling layers, ReLU activation, ensembling, bagging, decaying learning rates, dropout, batch normalization, and adam optimization. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## LOAD LIBRARIES"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-08-29T19:07:05.42508Z","iopub.status.busy":"2021-08-29T19:07:05.424855Z","iopub.status.idle":"2021-08-29T19:07:06.405244Z","shell.execute_reply":"2021-08-29T19:07:06.404495Z","shell.execute_reply.started":"2021-08-29T19:07:05.425025Z"},"trusted":true},"outputs":[],"source":["%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import LearningRateScheduler"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"cd31c62c12088bfa6f2b26dcecc714182627c767"},"source":["## Load Kaggle's 42,000 training images"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## LOAD THE DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d71b3fa2b10620dc8870352fc18d4548f824a88a","execution":{"iopub.execute_input":"2021-08-29T19:07:11.612724Z","iopub.status.busy":"2021-08-29T19:07:11.612433Z","iopub.status.idle":"2021-08-29T19:07:18.409471Z","shell.execute_reply":"2021-08-29T19:07:18.408738Z","shell.execute_reply.started":"2021-08-29T19:07:11.612664Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(\"../../assets/data/mnist_train.csv\")\n","test = pd.read_csv(\"../../assets/data/mnist_test.csv\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## PREPARE DATA FOR NEURAL NETWORK"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"b3c56055d1ba56d28d982f9647c33439c46753ff","execution":{"iopub.execute_input":"2021-08-29T19:07:25.457303Z","iopub.status.busy":"2021-08-29T19:07:25.457009Z","iopub.status.idle":"2021-08-29T19:07:26.187839Z","shell.execute_reply":"2021-08-29T19:07:26.186775Z","shell.execute_reply.started":"2021-08-29T19:07:25.457248Z"},"trusted":true},"outputs":[],"source":["Y_train = train[\"label\"]\n","X_train = train.drop(labels=[\"label\"], axis=1)\n","X_train = X_train / 255.0\n","\n","Y_test = test[\"label\"]\n","X_test = test.drop(labels=[\"label\"], axis=1)\n","X_test = X_test / 255.0\n","\n","X_train = X_train.values.reshape(-1, 28, 28, 1)\n","X_test = X_test.values.reshape(-1, 28, 28, 1)\n","Y_train = to_categorical(Y_train, num_classes=10)\n","Y_test = to_categorical(Y_test, num_classes=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"b95ca2c1e71cb5457684eff3c35bb8d68b4a0f97","execution":{"iopub.execute_input":"2021-08-29T19:07:28.931681Z","iopub.status.busy":"2021-08-29T19:07:28.931375Z","iopub.status.idle":"2021-08-29T19:07:30.013936Z","shell.execute_reply":"2021-08-29T19:07:30.012975Z","shell.execute_reply.started":"2021-08-29T19:07:28.931631Z"},"trusted":true},"outputs":[],"source":["# PREVIEW IMAGES\n","plt.figure(figsize=(15, 4.5))\n","for i in range(30):\n","    plt.subplot(3, 10, i + 1)\n","    plt.imshow(X_train[i].reshape((28, 28)), cmap=plt.cm.binary)\n","    plt.axis(\"off\")\n","plt.subplots_adjust(wspace=-0.1, hspace=-0.1)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"cfcb89d7d2dab632986e80d9f68d194c3c1c9e9f"},"source":["## Generate 25 million more images!!\n","by randomly rotating, scaling, and shifting Kaggle's 42,000 images."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Create more images via data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1e61e07d14b9b012748fdaac9eaf02e5263a475e","execution":{"iopub.execute_input":"2021-08-29T19:07:33.129772Z","iopub.status.busy":"2021-08-29T19:07:33.129278Z","iopub.status.idle":"2021-08-29T19:07:33.139004Z","shell.execute_reply":"2021-08-29T19:07:33.13814Z","shell.execute_reply.started":"2021-08-29T19:07:33.129571Z"},"trusted":true},"outputs":[],"source":["datagen = ImageDataGenerator(\n","    rotation_range=10, zoom_range=0.10, width_shift_range=0.1, height_shift_range=0.1\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Preview augmented images"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"fcf6daaae4424b95978856d7e75271c97b971c71","execution":{"iopub.execute_input":"2021-08-29T19:07:35.766934Z","iopub.status.busy":"2021-08-29T19:07:35.766646Z","iopub.status.idle":"2021-08-29T19:07:36.936847Z","shell.execute_reply":"2021-08-29T19:07:36.936027Z","shell.execute_reply.started":"2021-08-29T19:07:35.766878Z"},"trusted":true},"outputs":[],"source":["X_train3 = X_train[9,].reshape((1, 28, 28, 1))\n","Y_train3 = Y_train[9,].reshape((1, 10))\n","plt.figure(figsize=(15, 4.5))\n","for i in range(30):\n","    plt.subplot(3, 10, i + 1)\n","    X_train2, Y_train2 = datagen.flow(X_train3, Y_train3).next()\n","    plt.imshow(X_train2[0].reshape((28, 28)), cmap=plt.cm.binary)\n","    plt.axis(\"off\")\n","    if i == 9:\n","        X_train3 = X_train[11,].reshape((1, 28, 28, 1))\n","    if i == 19:\n","        X_train3 = X_train[18,].reshape((1, 28, 28, 1))\n","plt.subplots_adjust(wspace=-0.1, hspace=-0.1)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"9ea116cd3688cb26ac79b9fecc7309a1aebf3b63"},"source":["## Build 15 Convolutional Neural Networks!"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f6703f3f53c659e95579122755454899d842722a","execution":{"iopub.execute_input":"2021-08-29T19:07:42.889884Z","iopub.status.busy":"2021-08-29T19:07:42.8896Z","iopub.status.idle":"2021-08-29T19:07:56.725833Z","shell.execute_reply":"2021-08-29T19:07:56.72498Z","shell.execute_reply.started":"2021-08-29T19:07:42.889835Z"},"trusted":true},"outputs":[],"source":["# build convolutional neural networks\n","nets = 15\n","model = [0] * nets\n","for j in range(nets):\n","    model[j] = Sequential()\n","\n","    model[j].add(Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(28, 28, 1)))\n","    model[j].add(BatchNormalization())\n","    model[j].add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n","    model[j].add(BatchNormalization())\n","    model[j].add(\n","        Conv2D(32, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\")\n","    )\n","    model[j].add(BatchNormalization())\n","    model[j].add(Dropout(0.4))\n","\n","    model[j].add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n","    model[j].add(BatchNormalization())\n","    model[j].add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n","    model[j].add(BatchNormalization())\n","    model[j].add(\n","        Conv2D(64, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\")\n","    )\n","    model[j].add(BatchNormalization())\n","    model[j].add(Dropout(0.4))\n","\n","    model[j].add(Conv2D(128, kernel_size=4, activation=\"relu\"))\n","    model[j].add(BatchNormalization())\n","    model[j].add(Flatten())\n","    model[j].add(Dropout(0.4))\n","    model[j].add(Dense(10, activation=\"softmax\"))\n","\n","    # compile with adam optimizer and cross entropy cost\n","    model[j].compile(\n","        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n","    )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Architectural highlights"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"843d2cb58465b81404c47559ceaf96c139ff82da"},"source":["The CNNs in this kernel follow [LeNet5's][1] design (pictured above) with the following improvements:  \n","* Two stacked 3x3 filters replace the single 5x5 filters. These become nonlinear 5x5 convolutions\n","* A convolution with stride 2 replaces pooling layers. These become learnable pooling layers.\n","* ReLU activation replaces sigmoid.\n","* Batch normalization is added\n","* Dropout is added\n","* More feature maps (channels) are added\n","* An ensemble of 15 CNNs with bagging is used  \n","  \n","Experiments [(here)][2] show that each of these changes improve classification accuracy.\n","\n","[1]:http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n","[2]:https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"e433661c7762b947c0fbfc4ad3f5e1d2e056312c"},"source":["## Train 15 CNNs"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9f1dd8a54aa0fab8530c0095f7d4c4b35984ea6d","execution":{"iopub.execute_input":"2021-08-29T19:09:14.654661Z","iopub.status.busy":"2021-08-29T19:09:14.65437Z","iopub.status.idle":"2021-08-29T19:19:33.234441Z","shell.execute_reply":"2021-08-29T19:19:33.23336Z","shell.execute_reply.started":"2021-08-29T19:09:14.654605Z"},"trusted":true},"outputs":[],"source":["# decrease learning rate each epoch\n","annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95**x)\n","# train networks\n","history = [0] * nets\n","epochs = 45\n","for j in range(nets):\n","    X_train2, X_val2, Y_train2, Y_val2 = train_test_split(\n","        X_train, Y_train, test_size=0.1\n","    )\n","    history[j] = model[j].fit_generator(\n","        datagen.flow(X_train2, Y_train2, batch_size=64),\n","        epochs=epochs,\n","        steps_per_epoch=X_train2.shape[0] // 64,\n","        validation_data=(X_val2, Y_val2),\n","        callbacks=[annealer],\n","        verbose=0,\n","    )\n","    print(\n","        \"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n","            j + 1,\n","            epochs,\n","            max(history[j].history[\"accuracy\"]),\n","            max(history[j].history[\"val_accuracy\"]),\n","        )\n","    )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## How much more accuracy is possible?"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"372eaad5737aeef94f084c9c3317f6537db2cf2f"},"source":["Not much. Here are the best published MNIST classifiers found on the internet:\n","* 99.79% [Regularization of Neural Networks using DropConnect, 2013][1]\n","* 99.77% [Multi-column Deep Neural Networks for Image Classification, 2012][2]\n","* 99.77% [APAC: Augmented PAttern Classification with Neural Networks, 2015][3]\n","* 99.76% [Batch-normalized Maxout Network in Network, 2015][4]\n","* **99.75% [This Kaggle published kernel, 2018][12]**\n","* 99.73% [Convolutional Neural Network Committees, 2011][13]\n","* 99.71% [Generalized Pooling Functions in Convolutional Neural Networks, 2016][5]\n","* More examples: [here][7], [here][8], and [here][9]  \n","  \n","On Kaggle's website, there are no published kernels more accurate than 99.70% besides the one you're reading. The few you will find posted were trained on the full original MNIST dataset (of 70,000 images) which contains known labels for Kaggle's unknown \"test.csv\" images so those models aren't actually that accurate. For example, [one kernel achieves 100% accuracy][10] training on the original MNIST dataset.  \n","  \n","Below is a annotated histogram of Kaggle submission scores. Each bar has range 0.1%. There are spikes at 99.1% and 99.6% accuracy corresponding with using convolutional neural networks. Frequency count decreases as scores exceed 99.69%, hitting a low at 99.8% which is just past the highest possible accuracy. Then frequency count spikes again at accuracies of 99.9% and 100.0% corresponding to mistakenly training with the full original MNIST dataset.  \n","  \n","![KaggleMNISThist3](..//..//..//images/deep-learning/CNN/KaggleMNISThist3.png)\n","\n","[1]:https://cs.nyu.edu/~wanli/dropc/dropc.pdf\n","[2]:http://people.idsia.ch/~ciresan/data/cvpr2012.pdf\n","[3]:https://arxiv.org/abs/1505.03229\n","[4]:https://arxiv.org/abs/1511.02583\n","[5]:https://arxiv.org/abs/1509.08985\n","[7]:http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n","[8]:http://yann.lecun.com/exdb/mnist/\n","[9]:https://en.wikipedia.org/wiki/MNIST_database\n","[10]:https://www.kaggle.com/cdeotte/mnist-perfect-100-using-knn/\n","[12]:https://www.kaggle.com/cdeotte/35-million-images-0-99757-mnist\n","[13]:http://people.idsia.ch/~ciresan/data/icdar2011a.pdf\n","[14]:http://www.mva-org.jp/Proceedings/2015USB/papers/14-21.pdf"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"e716712230dda4fc08bc4705cfbbb3c9a9942228"},"source":["## CNN Performance\n","How can we evaluate the performance of a neural network? A trained neural network performs differently each time you train it since the weights are randomly initialized. Therefore, to assess a neural network's performance, we must train it many times and take an average of accuracy. The ensemble in this notebook was trained and evaluated 100 times!! (on the original MNIST dataset with 60k/10k split using the code template [here][1] on GitHub.) Below is a histogram of its accuracy.  \n","  \n","The maximum accuracy of an individual CNN was 99.81% with average accuracy 99.641% and standard deviation 0.047. The maximum accuracy of an ensemble of fifteen CNNs was 99.79% with average accuracy 99.745% and standard deviation 0.020.  \n","  \n","![histBoth5](../../../images/deep-learning/CNN/histBoth5.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Acknowledgments\n","\n","Thanks to [Chris Deotte](https://www.kaggle.com/cdeotte) for creating [25-million-images-0-99757-mnist](https://www.kaggle.com/code/aaahqw/25-million-images-0-99757-mnist). It inspires the majority of the content in this chapter."]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
